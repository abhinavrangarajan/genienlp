{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from genienlp.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genienlp.__main__ import *\n",
    "from genienlp.models.mqan_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(prog='genienlp')\n",
    "subparsers = parser.add_subparsers(dest='subcommand')\n",
    "for subcommand in subcommands:\n",
    "    helpstr, get_parser, command_fn = subcommands[subcommand]\n",
    "    get_parser(subparsers.add_parser(subcommand, help=helpstr))\n",
    "\n",
    "argv = parser.parse_args(['train',\n",
    " '--train_tasks',\n",
    " 'almond',\n",
    " '--train_iterations',\n",
    " '1000',\n",
    " '--embeddings',\n",
    " 'embeddings_dir',\n",
    " '--data',\n",
    " 'local_data',\n",
    " '--save',\n",
    " 'dummy_model_dir',\n",
    " '--no_commit',\n",
    " '--exist_ok',\n",
    " '--skip_cache',\n",
    " '--lambd',\n",
    " '0.5'])\n",
    "# subcommands[argv.subcommand][2](argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "genienlp.train - Arguments:\n",
      "{'aux_dataset': '',\n",
      " 'baseline': False,\n",
      " 'beta0': 0.9,\n",
      " 'cache': '.cache/',\n",
      " 'commit': '',\n",
      " 'confidence_mode': 'mean',\n",
      " 'curriculum_max_frac': 1.0,\n",
      " 'curriculum_rate': 0.1,\n",
      " 'curriculum_strategy': 'linear',\n",
      " 'data': './local_data',\n",
      " 'decoder_embeddings': 'glove+char',\n",
      " 'devices': [0],\n",
      " 'dimension': 200,\n",
      " 'dist_sync_file': './dummy_model_dir/distributed_sync_file',\n",
      " 'dropout_ratio': 0.2,\n",
      " 'embeddings': './embeddings_dir',\n",
      " 'encoder_embeddings': 'glove+char',\n",
      " 'exist_ok': True,\n",
      " 'grad_clip': 1.0,\n",
      " 'jump_start': 0,\n",
      " 'lambd': 0.5,\n",
      " 'load': None,\n",
      " 'log_dir': './dummy_model_dir',\n",
      " 'log_every': 100,\n",
      " 'lower': True,\n",
      " 'lr_rate': 0.001,\n",
      " 'max_answer_length': 50,\n",
      " 'max_generative_vocab': 50000,\n",
      " 'max_output_length': 100,\n",
      " 'max_to_keep': 5,\n",
      " 'max_train_context_length': 500,\n",
      " 'max_val_context_length': 500,\n",
      " 'model': 'Seq2Seq',\n",
      " 'n_jump_start': 0,\n",
      " 'num_print': 15,\n",
      " 'optimizer': 'adam',\n",
      " 'resume': False,\n",
      " 'rnn_dimension': 200,\n",
      " 'rnn_layers': 1,\n",
      " 'rnn_zero_state': 'zero',\n",
      " 'root': '.',\n",
      " 'save': './dummy_model_dir',\n",
      " 'save_every': 1000,\n",
      " 'seed': 123,\n",
      " 'seq2seq_decoder': 'MQANDecoder',\n",
      " 'seq2seq_encoder': 'MQANEncoder',\n",
      " 'skip_cache': True,\n",
      " 'subcommand': 'train',\n",
      " 'subsample': 20000000,\n",
      " 'tensorboard': True,\n",
      " 'tensorboard_dir': 'dummy_model_dir',\n",
      " 'timestamp': '2020-03-09T03:37:19.106328+00:00',\n",
      " 'train_batch_tokens': [9000],\n",
      " 'train_encoder_embeddings': False,\n",
      " 'train_iterations': [1000],\n",
      " 'train_task_names': ['almond'],\n",
      " 'train_tasks': [<genienlp.tasks.almond.Almond object at 0x1a2b814278>],\n",
      " 'trainable_decoder_embeddings': 0,\n",
      " 'transformer_heads': 3,\n",
      " 'transformer_hidden': 150,\n",
      " 'transformer_layers': 2,\n",
      " 'transformer_lr': True,\n",
      " 'transformer_lr_multiply': 1.0,\n",
      " 'use_curriculum': False,\n",
      " 'val_batch_size': [256],\n",
      " 'val_every': 1000,\n",
      " 'val_filter': True,\n",
      " 'val_task_names': ['almond'],\n",
      " 'val_tasks': [<genienlp.tasks.almond.Almond object at 0x1a2b8142e8>],\n",
      " 'vocab_tasks': None,\n",
      " 'warmup': 800,\n",
      " 'weight_decay': 0.0}\n",
      "genienlp.train - Loading almond\n",
      "genienlp.train - Adding almond to training datasets\n",
      "100%|█████████▉| 3644130/3649211 [01:49<00:00, 68828.29it/s]genienlp.train - almond has 3649211 training examples\n",
      "genienlp.train - Loading almond\n",
      "genienlp.train - Adding almond to validation datasets\n",
      "\n",
      "  0%|          | 0/1480 [00:00<?, ?it/s]\u001b[Agenienlp.train - almond has 1480 validation examples\n",
      "genienlp.train - Getting pretrained word vectors and pretrained models\n",
      "genienlp.train - Building vocabulary\n",
      "genienlp.train - Initializing encoder and decoder embeddings\n",
      "genienlp.train - Vocabulary has 208975 tokens\n",
      "genienlp.train - The first 200 tokens:\n",
      "genienlp.train - ['<unk>', '<pad>', '<init>', '<eos>', 'to', '=>', '(', ')', '=', 'from', 'translate', 'english', 'thingtalk', 'on', 'notify', 'the', 'monitor', '\"', 'now', 'of', 'a', '.', 'my', 'me', 'in', 'some', '1', 'DURATION_0', 'and', 'filter', 'i', 'when', ',', 'new', 'is', '==', 'base', 'interval', 'timer', 'time', 'NUMBER_0', '0', 'param:date:Date', 'an', 'with', 'location', 'for', 'changes', 'at', 'date', 'their', 'its', 'unit:day', 'uber', 'if', 'sunset', 'sunrise', 'edge', 'get', 'param:message:String', '@com.uber.price_estimate', 'thermostat', 'once', 'param:location:Location', 'show', 'temperature', '@org.thingpedia.weather.sunrise', 'unit:F', 'TIME_0', 'post', 'nest', 'send', 'place', 'files', 'LOCATION_0', '>=', 'set', 'alert', 'file', '@org.thingpedia.builtin.thingengine.builtin.say', 'change', 'what', 'location:current_location', 'than', 'youtube', '@security-camera.current_event', 'price', 'want', '=~', 'day', 'true', 'undefined', 'message', 'tell', 'camera', 'start_of', 'dropbox', 'whenever', 'one', 'estimate', 'certain', 'name', 'that', 'param:start:Location', '@thermostat.set_minmax_temperature', 'every', 'picture', '<=', 'unit:C', 'param:end:Location', 'something', 'security', '-', \"'s\", 'param:high:Measure(C)', 'times', 'almond', 'after', '+', 'param:low:Measure(C)', 'folder', 'zero', 'before', 'week', 'onedrive', 'end_of', 'unit:week', 'param:title:String', 'low', 'not', 'high', 'hey', 'know', 'channels', 'location:home', 'let', '@org.thingpedia.icalendar.list_events', 'location:work', 'notification', 'twitter', 'PATH_NAME_0', '@com.instagram.get_pictures', '@com.dropbox.list_folder', 'washington', 'here', 'instagram', '@com.fitbit.getsteps', 'fitbit', '@com.youtube.channels_by_category', 'give', 'there', 'month', 'google', 'unit:mon', 'drive', 'saying', 'home', 'attimer', 'param:category_id:Enum(best_of_youtube,recommended,paid,music,comedy,film_and_entertainment,gaming,beauty_and_fashion,from_tv,automotive,animation,sports,diy,tech,science,cooking,causes,news_and_politics,lifestyle)', 'between', '@org.thingpedia.weather.current', 'text', 'param:value:Measure(C)', '@com.google.drive.list_drive_files', 'daily', 'title', 'work', 'this', 'equal', 'any', 'steps', 'are', '@com.gmail.inbox', 'blog', 'updates', 'github', 'you', 'by', 'update', '^^tt:username', 'case', 'cost', 'order', 'last', 'where', 'am', 'having', 'pictures', '@thermostat.set_target_temperature', 'number', 'degrees', 'about', 'headline', 'param:file_size:Measure(byte)', 'fahrenheit', 'f', 'join', 'email', 'param:order_by:Enum(created_time_increasing,created_time_decreasing,modified_time_increasing,modified_time_decreasing,name_increasing,name_decreasing)', 'retrieve']\n",
      "genienlp.train - Preprocessing training data\n",
      "genienlp.train - almond has 3649211 examples\n",
      "genienlp.train - almond context lengths (min, mean, max): 2, 15, 40\n",
      "genienlp.train - almond question lengths (min, mean, max): 5, 5, 5\n",
      "genienlp.train - almond answer lengths (min, mean, max): 1, 14, 50\n",
      "genienlp.train - Tokenized examples:\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg tv should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: \u001b\n",
      "genienlp.train - Context: the volume of my lg televisions should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: the volume of my lg televisions should be turned down .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Context: decrease the volume of my lg tv for me .\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.lg.tv.webos2.lower_volume\n",
      "genienlp.train - Preprocessing validation data\n",
      "genienlp.train - almond has 1480 examples\n",
      "genienlp.train - almond context lengths (min, mean, max): 1, 7, 27\n",
      "genienlp.train - almond question lengths (min, mean, max): 5, 5, 5\n",
      "genienlp.train - almond answer lengths (min, mean, max): 3, 9, 40\n",
      "genienlp.train - Tokenized examples:\n",
      "genienlp.train - Context: quickly email a photo to yourself\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.gmail.send_picture\n",
      "genienlp.train - Context: if any new sms received , then send an email to gmail\n",
      "genienlp.train - Question: translate from english to thingtalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "genienlp.train - Answer: monitor ( @org.thingpedia.builtin.thingengine.phone.sms ) => @com.gmail.send_email\n",
      "genienlp.train - Context: send an email to someone when i leave work\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: edge ( monitor ( @org.thingpedia.builtin.thingengine.phone.get_gps ) ) on not param:location:Location == location:work => @com.gmail.send_email\n",
      "genienlp.train - Context: rain send an email alert\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: edge ( monitor ( @org.thingpedia.weather.current ) ) on param:status:Enum(raining,cloudy,sunny,snowy,sleety,drizzling,windy) == enum:raining => @com.gmail.send_email\n",
      "genienlp.train - Context: text john on gmail when i am approaching LOCATION_0\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: edge ( monitor ( @org.thingpedia.builtin.thingengine.phone.get_gps ) ) on param:location:Location == LOCATION_0 => @com.gmail.send_email param:to:Entity(tt:email_address) = \" john \" ^^tt:username\n",
      "genienlp.train - Context: send an email to john when i am near home\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: edge ( monitor ( @org.thingpedia.builtin.thingengine.phone.get_gps ) ) on param:location:Location == location:home => @com.gmail.send_email param:to:Entity(tt:email_address) = \" john \" ^^tt:username\n",
      "genienlp.train - Context: receive an email notification if motion is detected\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: monitor ( ( @security-camera.current_event ) filter param:has_motion:Boolean == true ) => @com.gmail.send_email\n",
      "genienlp.train - Context: receive the nasa picture of the day as an email\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: monitor ( @gov.nasa.apod ) => @com.gmail.send_picture on param:picture_url:Entity(tt:picture) = param:picture_url:Entity(tt:picture)\n",
      "genienlp.train - Context: send a copy of an sms sent to an email account\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: monitor ( @org.thingpedia.builtin.thingengine.phone.sms ) => @com.gmail.send_email on param:message:String = param:message:String\n",
      "genienlp.train - Context: quickly email people i am running late\n",
      "genienlp.train - Question: translate from english to thingtalk\n",
      "genienlp.train - Answer: now => @com.gmail.send_email param:message:String = \" i am running late \"\n",
      "genienlp.train - Processing\n",
      "genienlp.train - Initializing Writer\n",
      "genienlp.train - Initializing Seq2Seq\n",
      "genienlp.train - Seq2Seq has 14,519,503 parameters\n"
     ]
    }
   ],
   "source": [
    "args = arguments.post_parse(argv)\n",
    "if args is None:\n",
    "    raise Exception(\"What the fuck!!!\")\n",
    "\n",
    "set_seed(args)\n",
    "devices = init_devices(args, args.devices)\n",
    "logger = initialize_logger(args)\n",
    "logger.info(f'Arguments:\\n{pformat(vars(args))}')\n",
    "\n",
    "save_dict = None\n",
    "if args.load is not None:\n",
    "    logger.info(f'Loading vocab from {os.path.join(args.save, args.load)}')\n",
    "    save_dict = torch.load(os.path.join(args.save, args.load))\n",
    "numericalizer, encoder_embeddings, decoder_embeddings, train_sets, val_sets, aux_sets = prepare_data(args, logger)\n",
    "if (args.use_curriculum and aux_sets is None) or (not args.use_curriculum and len(aux_sets)):\n",
    "    logging.error('sth unpleasant is happening with curriculum')\n",
    "\n",
    "logger.info(f'Processing')\n",
    "logger.start = time.time()\n",
    "\n",
    "if hasattr(args, 'tensorboard') and args.tensorboard:\n",
    "    logger.info(f'Initializing Writer')\n",
    "    writer = SummaryWriter(log_dir=args.tensorboard_dir)\n",
    "else:\n",
    "    writer = None\n",
    "\n",
    "model = init_model(args, numericalizer, encoder_embeddings, decoder_embeddings, devices, logger)\n",
    "opt, lr_scheduler = init_opt(args, model, logger)\n",
    "start_iteration = 1\n",
    "\n",
    "if save_dict is not None:\n",
    "    logger.info(f'Loading model from {os.path.join(args.save, args.load)}')\n",
    "    save_dict = torch.load(os.path.join(args.save, args.load))\n",
    "    model.load_state_dict(save_dict['model_state_dict'])\n",
    "    if args.resume:\n",
    "        logger.info(f'Resuming Training from {os.path.splitext(args.load)[0]}_optim.pth')\n",
    "        opt_state_dict = torch.load(os.path.join(args.save, f'{os.path.splitext(args.load)[0]}_optim.pth'))\n",
    "        start_iteration = opt_state_dict.pop('start_iteration')\n",
    "        logger.info(f'Starting iteration is {start_iteration}')\n",
    "        opt.load_state_dict(opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = args.train_iterations\n",
    "log_every=args.log_every\n",
    "val_every=args.val_every\n",
    "save_every=args.save_every\n",
    "rounds=len(train_sets) > 1\n",
    "rnd=1\n",
    "best_decascore=save_dict.get('best_decascore') if save_dict is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "genienlp.train - Preparing iterators\n",
      "genienlp.train - Begin Training\n"
     ]
    }
   ],
   "source": [
    "local_loss, num_examples, len_contexts, len_answers, iteration = 0, 0, 0, 0, start_iteration\n",
    "\n",
    "train_iter_deep = deepcopy(train_iterations)\n",
    "\n",
    "task_iteration = dict()\n",
    "task_done = dict()\n",
    "task_fraction = dict()\n",
    "\n",
    "for task in args.train_tasks:\n",
    "    task_iteration[task] = 1\n",
    "    task_done[task] = False\n",
    "    task_fraction[task] = 0.0\n",
    "\n",
    "saver = Saver(args.log_dir, args.max_to_keep)\n",
    "epoch = 0\n",
    "\n",
    "logger.info(f'Preparing iterators')\n",
    "main_device = devices[0]\n",
    "train_iters = [(task, make_data_loader(x, numericalizer, tok, main_device, train=True))\n",
    "               for task, x, tok in zip(args.train_tasks, train_sets, args.train_batch_tokens)]\n",
    "train_iters = [(task, iter(train_iter)) for task, train_iter in train_iters]\n",
    "\n",
    "val_iters = [(task, make_data_loader(x, numericalizer, bs, main_device, train=False))\n",
    "             for task, x, bs in zip(args.val_tasks, val_sets, args.val_batch_size)]\n",
    "\n",
    "if args.use_curriculum:\n",
    "    aux_iters = [(name, make_data_loader(x, numericalizer, tok, main_device, train=True))\n",
    "                 for name, x, tok in zip(args.train_tasks, aux_sets, args.train_batch_tokens)]\n",
    "    aux_iters = [(task, iter(aux_iter)) for task, aux_iter in aux_iters]\n",
    "\n",
    "zero_loss = 0\n",
    "logger.info(f'Begin Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = train_iter_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# while True:\n",
    "#     try:\n",
    "#         cnt += len(next(train_iter).example_id)\n",
    "#         print(cnt)\n",
    "#     except:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 0\n",
      "0\n",
      "0\n",
      "scores.shape:  torch.Size([120, 16, 50000])\n",
      "confidence.shape:  torch.Size([120, 16, 1])\n",
      "Iteration Number: 1\n",
      "0\n",
      "120\n",
      "scores.shape:  torch.Size([94, 20, 50000])\n",
      "confidence.shape:  torch.Size([94, 20, 1])\n",
      "Iteration Number: 2\n",
      "0\n",
      "214\n",
      "scores.shape:  torch.Size([138, 14, 50000])\n",
      "confidence.shape:  torch.Size([138, 14, 1])\n",
      "Iteration Number: 3\n",
      "0\n",
      "352\n",
      "scores.shape:  torch.Size([78, 24, 50000])\n",
      "confidence.shape:  torch.Size([78, 24, 1])\n",
      "Iteration Number: 4\n",
      "0\n",
      "430\n",
      "scores.shape:  torch.Size([94, 20, 50000])\n",
      "confidence.shape:  torch.Size([94, 20, 1])\n",
      "Iteration Number: 5\n",
      "0\n",
      "524\n",
      "scores.shape:  torch.Size([150, 13, 50000])\n",
      "confidence.shape:  torch.Size([150, 13, 1])\n",
      "Iteration Number: 6\n",
      "0\n",
      "674\n",
      "scores.shape:  torch.Size([128, 15, 50000])\n",
      "confidence.shape:  torch.Size([128, 15, 1])\n",
      "Iteration Number: 7\n",
      "0\n",
      "802\n",
      "scores.shape:  torch.Size([120, 16, 50000])\n",
      "confidence.shape:  torch.Size([120, 16, 1])\n",
      "Iteration Number: 8\n",
      "0\n",
      "922\n",
      "scores.shape:  torch.Size([94, 20, 50000])\n",
      "confidence.shape:  torch.Size([94, 20, 1])\n",
      "Iteration Number: 9\n",
      "0\n",
      "1016\n",
      "scores.shape:  torch.Size([120, 16, 50000])\n",
      "confidence.shape:  torch.Size([120, 16, 1])\n",
      "Iteration Number: 10\n",
      "0\n",
      "1136\n",
      "scores.shape:  torch.Size([62, 30, 50000])\n",
      "confidence.shape:  torch.Size([62, 30, 1])\n",
      "Iteration Number: 11\n",
      "0\n",
      "1198\n",
      "scores.shape:  torch.Size([86, 21, 50000])\n",
      "confidence.shape:  torch.Size([86, 21, 1])\n",
      "Iteration Number: 12\n",
      "0\n",
      "1284\n",
      "scores.shape:  torch.Size([65, 28, 50000])\n",
      "confidence.shape:  torch.Size([65, 28, 1])\n",
      "Iteration Number: 13\n",
      "0\n",
      "1349\n",
      "scores.shape:  torch.Size([78, 24, 50000])\n",
      "confidence.shape:  torch.Size([78, 24, 1])\n",
      "Iteration Number: 14\n",
      "0\n",
      "1427\n",
      "scores.shape:  torch.Size([94, 20, 50000])\n",
      "confidence.shape:  torch.Size([94, 20, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b4d2eba6b6b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# param update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 loss, grad_norm = step(model, batch, iteration, opt, lr_scheduler=lr_scheduler,\n\u001b[0;32m---> 96\u001b[0;31m                                        grad_clip=args.grad_clip, logger=logger)\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     logger.info(\n",
      "\u001b[0;32m~/Documents/genienlp/genienlp/train.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(model, batch, iteration, opt, lr_scheduler, grad_clip, logger)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got NaN loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for iter_no in range(500):\n",
    "    print(\"Iteration Number: {}\".format(iter_no))\n",
    "    cnt = 0\n",
    "    for task_idx, (task, train_iter) in enumerate(train_iters):\n",
    "        print(task_idx)\n",
    "        task_iterations = train_iterations[task_idx] if train_iterations is not None else None\n",
    "        if task_iterations == 0:\n",
    "            continue\n",
    "\n",
    "        if task_iterations is not None and task_iteration[task] > task_iterations:\n",
    "            task_done[task] = True\n",
    "            continue\n",
    "\n",
    "        if args.use_curriculum:\n",
    "            aux_iter = aux_iters[task_idx][1]\n",
    "            prob = np.random.choice(['train', 'aux'], p=[1 - task_fraction[task], task_fraction[task]])\n",
    "            if prob == 'aux':\n",
    "                batch = next(aux_iter)\n",
    "            else:\n",
    "                assert prob == 'train'\n",
    "                batch = next(train_iter)\n",
    "\n",
    "        else:\n",
    "            batch = next(train_iter)\n",
    "            cnt += num_examples\n",
    "            print(cnt)\n",
    "\n",
    "        # run only once\n",
    "        for _ in range(1):\n",
    "            if not args.resume or iteration > start_iteration:\n",
    "                task_progress = f'{task_iteration[task]}/{task_iterations}:' if task_iterations is not None else ''\n",
    "                round_progress = f'round_{rnd}:' if rounds else ''\n",
    "\n",
    "                # validate\n",
    "                deca_score = None\n",
    "                if (val_every is not None and\n",
    "                        ((iteration % args.val_every == 0 % args.val_every) or\n",
    "                         (args.load and iteration == start_iteration + 1))):\n",
    "\n",
    "                    deca_score = 0\n",
    "                    for val_task_idx, (val_task, val_iter) in enumerate(val_iters):\n",
    "                        val_loss, metric_dict = validate(val_task, val_iter, model, logger, numericalizer,\n",
    "                                                         iteration, num_print=args.num_print, args=args)\n",
    "                        if val_loss is not None:\n",
    "                            log_entry = f'{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task.name}:{task_progress}val_{val_task.name}:val_loss{val_loss.item():.4f}:'\n",
    "                            writer.add_scalar(f'loss/{val_task.name}/val', val_loss.item(), iteration)\n",
    "                        else:\n",
    "                            log_entry = f'{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task.name}:{task_progress}val_{val_task.name}:'\n",
    "\n",
    "                        metric_entry = ''\n",
    "                        for metric_key, metric_value in metric_dict.items():\n",
    "                            metric_entry += f'{metric_key}_{metric_value:.2f}:'\n",
    "                        metric_entry = metric_entry[:-1]\n",
    "\n",
    "                        deca_score += metric_dict[val_task.metrics[0]]\n",
    "\n",
    "                        # val log\n",
    "                        logger.info(log_entry + metric_entry)\n",
    "                        if writer is not None:\n",
    "                            for metric_key, metric_value in metric_dict.items():\n",
    "                                writer.add_scalar(f'{val_task.name}/{metric_key}/val', metric_value, iteration)\n",
    "                    if writer is not None:\n",
    "                        writer.add_scalar('deca/val', deca_score, iteration)\n",
    "                    logger.info(\n",
    "                        f'{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task.name}:{task_progress}val_deca:deca_{deca_score:.2f}')\n",
    "\n",
    "                # saving\n",
    "                if save_every is not None and (iteration % args.save_every == 0):\n",
    "                    should_save_best = False\n",
    "                    if deca_score is not None and (best_decascore is None or best_decascore < deca_score):\n",
    "                        best_decascore = deca_score\n",
    "                        should_save_best = True\n",
    "\n",
    "                    # punch through the nn.DataParallel to access the real model, otherwise we won't be able\n",
    "                    # to load this model later\n",
    "                    model_state_dict = model.module.state_dict()\n",
    "                    model_state_dict = {k: v.cpu() for k, v in model_state_dict.items()}\n",
    "\n",
    "                    save_model_state_dict = {\n",
    "                        'model_state_dict': model_state_dict,\n",
    "                        'best_decascore': best_decascore\n",
    "                    }\n",
    "                    save_opt_state_dict = opt.state_dict()\n",
    "                    save_opt_state_dict.update({'start_iteration': iteration})\n",
    "\n",
    "                    saver.save(save_model_state_dict, save_opt_state_dict, global_step=iteration)\n",
    "                    if should_save_best:\n",
    "                        logger.info(\n",
    "                            f'{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task.name}:{task_progress}found new best model')\n",
    "                        torch.save(save_model_state_dict, os.path.join(args.log_dir, 'best.pth'))\n",
    "                        torch.save(save_opt_state_dict, os.path.join(args.log_dir, 'best_optim.pth'))\n",
    "\n",
    "                # param update\n",
    "                loss, grad_norm = step(model, batch, iteration, opt, lr_scheduler=lr_scheduler,\n",
    "                                       grad_clip=args.grad_clip, logger=logger)\n",
    "                if loss is None:\n",
    "                    logger.info(\n",
    "                        'Encountered NAN loss during training... Continue training ignoring the current batch')\n",
    "                    continue\n",
    "                if loss < 1e-5:\n",
    "                    zero_loss += 1\n",
    "                    if zero_loss >= 100:\n",
    "                        logger.info('Found loss less than 1e-5 for 100 steps, stopping.')\n",
    "                        raise Exception(\"DONE!!!!!\")\n",
    "                else:\n",
    "                    zero_loss = 0\n",
    "\n",
    "                # update curriculum fraction\n",
    "                if args.use_curriculum:\n",
    "                    task_fraction[task] = update_fraction(args, task_iteration[task])\n",
    "\n",
    "                # train metrics\n",
    "                local_loss += loss\n",
    "\n",
    "                # train logs\n",
    "                num_examples += batch.context.value.size(0)\n",
    "                len_contexts += batch.context.value.size(1)\n",
    "                len_answers += batch.answer.value.size(1)\n",
    "\n",
    "                if log_every is not None and (iteration % log_every == 0 % log_every):\n",
    "                    local_loss /= args.log_every\n",
    "                    num_examples /= args.log_every\n",
    "                    len_contexts /= args.log_every\n",
    "                    len_answers /= args.log_every\n",
    "                    avg_batch_size = f'avbatch_{num_examples:.0f}_{len_contexts:.0f}_{len_answers:.0f}:'\n",
    "                    logger.info(\n",
    "                        f'{args.timestamp}:{elapsed_time(logger)}:iteration_{iteration}:{round_progress}train_{task.name}:{task_progress}{avg_batch_size}loss_{local_loss:.4f}')\n",
    "                    num_examples = 0\n",
    "                    len_contexts = 0\n",
    "                    len_answers = 0\n",
    "\n",
    "                    if writer is not None:\n",
    "                        writer.add_scalar(f'loss/{task.name}/train', local_loss, iteration)\n",
    "                        writer.add_scalar(f'training/loss/{task.name}', local_loss, iteration)\n",
    "\n",
    "                        if lr_scheduler is not None:\n",
    "                            writer.add_scalar(f'training/lr', lr_scheduler.get_last_lr(), iteration)\n",
    "                        else:\n",
    "                            writer.add_scalar(f'training/lr', args.lr_rate)\n",
    "                        if grad_norm is not None:\n",
    "                            writer.add_scalar(f'training/norm', grad_norm, iteration)\n",
    "\n",
    "                    local_loss = 0\n",
    "                    num_examples = 0\n",
    "\n",
    "            # book keeping\n",
    "            task_iteration[task] += 1\n",
    "            iteration += 1\n",
    "        \n",
    "    # book keeping\n",
    "    epoch += 1\n",
    "    rnd += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
